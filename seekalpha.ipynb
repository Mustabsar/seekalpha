{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/python3\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import os\n",
    "from time import sleep\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Supportive Functions\n",
    "def load_web(link):\n",
    "    \"\"\"\n",
    "    General: Load the webpage from a link. Return a soup project\n",
    "    \"\"\"\n",
    "    \n",
    "    headers = {\n",
    "    \"Accept\": \"text/html,application/xhtml+xml,application/xml; \"\n",
    "              \"q=0.9,image/webp,*/*;q=0.8\",\n",
    "    \"Accept-Encoding\": \"text/html\",\n",
    "    \"Accept-Language\": \"en-US,en;q=0.8,zh-CN;q=0.6,zh;q=0.4,zh-TW;q=0.2\",\n",
    "    \"Content-Type\": \"application/x-www-form-urlencoded\",\n",
    "    \"User-Agent\": \"Mozilla/5.0 (X11; Linux i686) AppleWebKit/537.36 \"\n",
    "                  \"(KHTML, like Gecko) Chrome/32.0.1700.77 Safari/537.36\"\n",
    "    }\n",
    "\n",
    "    keepit = 0\n",
    "    while keepit == 0:\n",
    "        try:\n",
    "            page = requests.get(link, headers=headers, timeout=20)\n",
    "            soup = BeautifulSoup(page.text, \"lxml\")\n",
    "            keepit = 1\n",
    "        except requests.exceptions.Timeout:\n",
    "            print('    [Error] Timeout! Retrying...')\n",
    "            print('    ' + link)\n",
    "\n",
    "    return soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Function to load the article\n",
    "def extract_link(page):\n",
    "    \"\"\"\n",
    "    To download the earning transcripts from seeking alpha\n",
    "    \"\"\"\n",
    "    print('Analysing page ' + str(page))\n",
    "    link = 'http://seekingalpha.com/earnings/earnings-call-transcripts/' + str(page)\n",
    "    dat = load_web(link)\n",
    "\n",
    "    arts = dat.find_all('a', class_ = 'dashboard-article-link')\n",
    "    arts = [{'link':'http://seekingalpha.com' + x['href'] + '?part=single',\n",
    "             'title':x.text} for x in arts]\n",
    "    \n",
    "    return(arts)\n",
    "    \n",
    "def extract_art(arts):\n",
    "    \"\"\"\n",
    "    Extract the transcripts from SA\n",
    "    Need the macro variable DB\n",
    "    \"\"\"\n",
    "    print('Downloading ' + str(len(arts)) + ' articles...')\n",
    "    output = []\n",
    "    for i in arts:\n",
    "        if check_status(i):\n",
    "            dat2 = load_web(i['link'])\n",
    "            article = dat2.find('div', class_ = 'sa-art').text\n",
    "            ticker = re.search(r'\\(\\w+\\)', i['title']).group(0)\n",
    "            paper = {'title':i['title'],\n",
    "                     'ticker':ticker,\n",
    "                     'text':article,\n",
    "                     'link':i['link']}\n",
    "            output.append(paper)\n",
    "            sleep(3)\n",
    "    \n",
    "    with open(DB, 'rt') as f:\n",
    "        old = json.load(f)\n",
    "    \n",
    "    print('Writing to database ...')\n",
    "    with open(DB, 'wt') as f:\n",
    "        json.dump(old + output, f)\n",
    "        \n",
    "def check_status(art):\n",
    "    \"\"\"\n",
    "    Check whether the article already exists in our DB\n",
    "    Needs the macro variable LINKPOOL\n",
    "    \"\"\"\n",
    "    if art['link'] in set(LINKPOOL):\n",
    "        return False\n",
    "    else:\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Main\n",
    "## Setup\n",
    "LDB = 'links.txt'\n",
    "DB = 'db.json'\n",
    "with open(DB, 'rt') as f:\n",
    "    db = json.load(f)\n",
    "LINKPOOL = [x['link'] for x in db]\n",
    "\n",
    "## Parsing Links\n",
    "for i in range(1, 4151):\n",
    "    newLink = extract_link(i)\n",
    "    with open(LDB, 'rt') as f:\n",
    "        oldLink = json.load(f)\n",
    "    with open(LDB, 'wt') as f:\n",
    "        json.dump(oldLink + newLink, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
