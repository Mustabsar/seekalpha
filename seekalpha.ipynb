{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/python3\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import os\n",
    "from time import sleep\n",
    "import json\n",
    "import sqlite3\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Supportive Functions\n",
    "def load_web(link):\n",
    "    \"\"\"\n",
    "    General: Load the webpage from a link. Return a soup project\n",
    "    \"\"\"\n",
    "    \n",
    "    headers = {\n",
    "    \"Accept\": \"text/html,application/xhtml+xml,application/xml; \"\n",
    "              \"q=0.9,image/webp,*/*;q=0.8\",\n",
    "    \"Accept-Encoding\": \"text/html\",\n",
    "    \"Accept-Language\": \"en-US,en;q=0.8,zh-CN;q=0.6,zh;q=0.4,zh-TW;q=0.2\",\n",
    "    \"Content-Type\": \"application/x-www-form-urlencoded\",\n",
    "    \"User-Agent\": \"Mozilla/5.0 (X11; Linux i686) AppleWebKit/537.36 \"\n",
    "                  \"(KHTML, like Gecko) Chrome/32.0.1700.77 Safari/537.36\"\n",
    "    }\n",
    "\n",
    "    keepit = 0\n",
    "    while keepit == 0:\n",
    "        try:\n",
    "            page = requests.get(link, headers=headers, timeout=20)\n",
    "            soup = BeautifulSoup(page.text, \"lxml\")\n",
    "            keepit = 1\n",
    "        except requests.exceptions.Timeout:\n",
    "            print('    [Error] Timeout! Retrying...')\n",
    "            print('    ' + link)\n",
    "\n",
    "    return soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Function to load the article\n",
    "def extract_link(page):\n",
    "    \"\"\"\n",
    "    To download the earning transcripts from seeking alpha\n",
    "    \"\"\"\n",
    "    print('Analysing page ' + str(page))\n",
    "    link = 'http://seekingalpha.com/earnings/earnings-call-transcripts/' + str(page)\n",
    "    dat = load_web(link)\n",
    "\n",
    "    arts = dat.find_all('a', class_ = 'dashboard-article-link')\n",
    "    if re.search(r'\\(.*?\\)', x.text):\n",
    "        ticker = re.search(r'\\(\\w+\\)', x.text).group(0)\n",
    "    else:\n",
    "        ticker = ''\n",
    "    arts = [{'link':'http://seekingalpha.com' + x['href'] + '?part=single',\n",
    "             'title':x.text,\n",
    "             'ticker':ticker,\n",
    "             'file':0} for x in arts]\n",
    "    \n",
    "    return(arts)\n",
    "    \n",
    "    \n",
    "def extract_art(i):\n",
    "    \"\"\"\n",
    "    Extract the transcripts from SA\n",
    "    Need the macro variable DB\n",
    "    \"\"\"\n",
    "    dat2 = load_web(i['link'])\n",
    "    try:\n",
    "        article = dat2.find('div', class_ = 'sa-art').text\n",
    "    except AttributeError:\n",
    "        article = ''\n",
    "        \n",
    "    return article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Parsing Links\n",
    "def collect_links(DB):\n",
    "    \"\"\"\n",
    "    A wrapper function to collect the links\n",
    "    \"\"\"\n",
    "    i = 1\n",
    "    while i <= 4151:\n",
    "        try:\n",
    "            newLink = extract_link(i)\n",
    "            with open(DB, 'rt') as f:\n",
    "                oldLink = json.load(f)\n",
    "            with open(DB, 'wt') as f:\n",
    "                json.dump(oldLink + newLink, f)\n",
    "            i = i + 1\n",
    "            sleep(random.randint(2, 5))\n",
    "        except requests.exceptions.ConnectionError:\n",
    "            sleep(60)\n",
    "        except requests.exceptions.ChunkedEncodingError:\n",
    "            sleep(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Downloading module\n",
    "def download_trans(DB):\n",
    "    \"\"\"\n",
    "    A wrapper function to download the transcripts\n",
    "    \"\"\"\n",
    "    # setup\n",
    "    print('Setting up environments...')\n",
    "    with open(DB, 'rt') as f:\n",
    "        db = json.load(f)\n",
    "\n",
    "    old = [x for x in db if x['file'] != 0]\n",
    "    todown = [x for x in db if x['file'] == 0]\n",
    "    id = max([x['file'] for x in db]) + 1\n",
    "        \n",
    "    # downloader\n",
    "    i = 0\n",
    "    batch = 0\n",
    "    print('Downloading ' + str(len(todown)) + ' files...')\n",
    "    while i < len(todown):\n",
    "        print('    ' + str(i + 1) + ' file')\n",
    "        try:\n",
    "            paper = extract_art(todown[i])\n",
    "            if paper != '':\n",
    "                with open('trs/' + str(id) + '.txt', 'wt') as f:\n",
    "                    f.write(paper)\n",
    "                todown[i]['file'] = id\n",
    "            else:\n",
    "                todown[i]['file'] = -1\n",
    "\n",
    "            id = id + 1\n",
    "            i = i + 1\n",
    "            batch = batch + 1\n",
    "            \n",
    "            if batch >= 20:         \n",
    "                with open(DB, 'wt') as f:\n",
    "                    json.dump(old + todown, f)\n",
    "                print('    Updated the database')\n",
    "                batch = 0\n",
    "            \n",
    "            sleep(random.randint(1, 2))\n",
    "            \n",
    "        except requests.exceptions.ConnectionError:\n",
    "            sleep(60)\n",
    "        except requests.exceptions.ChunkedEncodingError:\n",
    "            sleep(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Main\n",
    "DB = 'db.json'\n",
    "\n",
    "# collect_links(DB)\n",
    "download_trans(DB)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
